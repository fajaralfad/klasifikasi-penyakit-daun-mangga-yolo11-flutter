{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8DgYySRipOwiCVtqC/8Y1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fajaralfad/klasifikasi-penyakit-daun-mangga-yolo11-flutter/blob/main/Model_Deteksi_Penyakit_Mangga_YoloV11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting data**"
      ],
      "metadata": {
        "id": "s0J3uF_rJfRB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XnFVeRYCQhg",
        "outputId": "0380e2fc-0aab-429e-f1e1-794f52dabbad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/content/drive/MyDrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii_PMLWqMTmt",
        "outputId": "b4e775ad-b8a8-4304-f198-9842f1bee0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Video dan Foto', 'DeteksiPenyakitDaunMangga', 'data']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/drive/MyDrive/DeteksiPenyakitDaunMangga\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozy6AKJZMZcT",
        "outputId": "b0a535a7-fd30-43bd-b9d1-34fa706ca7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/drive/MyDrive/DeteksiPenyakitDaunMangga/data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuowSWARMcyo",
        "outputId": "a82be3a9-ca65-48c7-b784-98bfd0c954c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MangoLeafBD', 'Model_Deteksi_Penyakit_Mangga_YoloV11.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/drive/MyDrive/DeteksiPenyakitDaunMangga/data/MangoLeafBD\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2McysOasMiBs",
        "outputId": "d89688cc-b8e3-48bb-a3fe-3d209bcfec79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Anthracnose',\n",
              " 'Die Back',\n",
              " 'Cutting Weevil',\n",
              " 'Bacterial Canker',\n",
              " 'Powdery Mildew',\n",
              " 'Healthy',\n",
              " 'Sooty Mould',\n",
              " 'Gall Midge']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0x3i-6z5EDSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/drive/MyDrive/DeteksiPenyakitDaunMangga/data/MangoLeafBD\"\n",
        "base_dir = \"/content/drive/MyDrive/data/MangoLeafBD_Split\""
      ],
      "metadata": {
        "id": "3yus6yonGPka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in ['train', 'val', 'test']:\n",
        "    os.makedirs(os.path.join(base_dir, split), exist_ok=True)"
      ],
      "metadata": {
        "id": "s9rZ0InVJdLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil semua nama kelas (folder)\n",
        "classes = os.listdir(dataset_dir)\n",
        "\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(dataset_dir, cls)\n",
        "    if not os.path.isdir(cls_path):\n",
        "        continue  # skip file non-folder\n",
        "\n",
        "    imgs = os.listdir(cls_path)\n",
        "\n",
        "    # Split 70% train, 15% val, 15% test\n",
        "    train_files, temp_files = train_test_split(imgs, test_size=0.3, random_state=42)\n",
        "    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Copy ke folder tujuan\n",
        "    for split, files in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n",
        "        split_dir = os.path.join(base_dir, split, cls)\n",
        "        os.makedirs(split_dir, exist_ok=True)\n",
        "        for f in files:\n",
        "            shutil.copy(os.path.join(cls_path, f), os.path.join(split_dir, f))\n",
        "\n",
        "print(\"Dataset berhasil di-split menjadi train, val, dan test.\")\n"
      ],
      "metadata": {
        "id": "rPpL0KbkJqLX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in ['train', 'val', 'test']:\n",
        "    path = os.path.join(base_dir, split)\n",
        "    total_images = sum([len(files) for _, _, files in os.walk(path)])\n",
        "    print(f\"{split}: {total_images} gambar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BxmYMNKNXKI",
        "outputId": "625dcd1f-aae9-40c2-8305-e2aad45f2338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 2800 gambar\n",
            "val: 600 gambar\n",
            "test: 600 gambar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model dengan YOLOv11**"
      ],
      "metadata": {
        "id": "4CCO5GKLRMFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "daOl2x5AWdRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.data.dataset import ClassificationDataset\n",
        "from ultralytics.models.yolo.classify import ClassificationTrainer, ClassificationValidator\n",
        "\n",
        "\n",
        "class CustomizedDataset(ClassificationDataset):\n",
        "    \"\"\"A customized dataset class for image classification with enhanced data augmentation transforms.\"\"\"\n",
        "\n",
        "    def __init__(self, root: str, args, augment: bool = False, prefix: str = \"\"):\n",
        "        \"\"\"Initialize a customized classification dataset with enhanced data augmentation transforms.\"\"\"\n",
        "        super().__init__(root, args, augment, prefix)\n",
        "\n",
        "        # Enhanced training transforms\n",
        "        train_transforms = T.Compose(\n",
        "            [\n",
        "                T.Resize((args.imgsz, args.imgsz)),\n",
        "                T.RandomHorizontalFlip(p=0.5),\n",
        "                T.RandomVerticalFlip(p=0.2),\n",
        "                T.RandomRotation(degrees=15),\n",
        "                T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "                T.RandAugment(num_ops=2, magnitude=9, interpolation=T.InterpolationMode.BILINEAR),\n",
        "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                T.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3), inplace=True),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        val_transforms = T.Compose(\n",
        "            [\n",
        "                T.Resize((args.imgsz, args.imgsz)),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ]\n",
        "        )\n",
        "        self.torch_transforms = train_transforms if augment else val_transforms\n",
        "\n",
        "\n",
        "class CustomizedTrainer(ClassificationTrainer):\n",
        "    \"\"\"A customized trainer class for YOLO classification models with enhanced dataset handling.\"\"\"\n",
        "\n",
        "    def build_dataset(self, img_path: str, mode: str = \"train\", batch=None):\n",
        "        \"\"\"Build a customized dataset for classification training and the validation during training.\"\"\"\n",
        "        return CustomizedDataset(root=img_path, args=self.args, augment=mode == \"train\", prefix=mode)\n",
        "\n",
        "\n",
        "class CustomizedValidator(ClassificationValidator):\n",
        "    \"\"\"A customized validator class for YOLO classification models with enhanced dataset handling.\"\"\"\n",
        "\n",
        "    def build_dataset(self, img_path: str, mode: str = \"train\"):\n",
        "        \"\"\"Build a customized dataset for classification standalone validation.\"\"\"\n",
        "        return CustomizedDataset(root=img_path, args=self.args, augment=mode == \"train\", prefix=self.args.split)\n",
        "\n",
        "\n",
        "model = YOLO(\"yolo11n-cls.pt\")\n",
        "\n",
        "# Training dengan parameter minimal (augmentation sudah di CustomizedDataset)\n",
        "model.train(\n",
        "    data=\"/content/drive/MyDrive/data/MangoLeafBD_Split\",\n",
        "    trainer=CustomizedTrainer,\n",
        "\n",
        "    # Basic parameters\n",
        "    epochs=50,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "\n",
        "    # Regularization\n",
        "    dropout=0.3,\n",
        "    weight_decay=0.0005,\n",
        "\n",
        "    # Learning rate\n",
        "    lr0=0.001,\n",
        "    lrf=0.01,\n",
        "    momentum=0.937,\n",
        "    optimizer='AdamW',\n",
        "\n",
        "    # Early stopping\n",
        "    patience=20,\n",
        "\n",
        "    # Validation & saving\n",
        "    val=True,\n",
        "    plots=True,\n",
        "    save=True,\n",
        "    save_period=10,\n",
        "\n",
        "    # Hardware\n",
        "    device=0,\n",
        "    workers=8,\n",
        "\n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "    deterministic=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "results = model.val(\n",
        "    data=\"/content/drive/MyDrive/data/MangoLeafBD_Split\",\n",
        "    validator=CustomizedValidator,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "    split='test'\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"Top-1 Accuracy: {results.top1:.4f}\")\n",
        "print(f\"Top-5 Accuracy: {results.top5:.4f}\")"
      ],
      "metadata": {
        "id": "JCihkRJ5WqnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scp -r /content/runs '/content/drive/MyDrive/data/MangoLeafBD_Split'"
      ],
      "metadata": {
        "id": "mxKCwJ1XPFqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "def print_best_metrics(results_csv_path):\n",
        "    df = pd.read_csv(results_csv_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    best_val_loss_idx = df['val/loss'].idxmin()\n",
        "    best_top1_idx = df['metrics/accuracy_top1'].idxmax()\n",
        "    best_top5_idx = df['metrics/accuracy_top5'].idxmax()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BEST METRICS SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nBest Validation Loss:\")\n",
        "    print(f\"   Epoch: {df.loc[best_val_loss_idx, 'epoch']:.0f}\")\n",
        "    print(f\"   Val Loss: {df.loc[best_val_loss_idx, 'val/loss']:.4f}\")\n",
        "    print(f\"   Train Loss: {df.loc[best_val_loss_idx, 'train/loss']:.4f}\")\n",
        "    print(f\"   Top-1 Acc: {df.loc[best_val_loss_idx, 'metrics/accuracy_top1']:.4f}\")\n",
        "    print(f\"   Top-5 Acc: {df.loc[best_val_loss_idx, 'metrics/accuracy_top5']:.4f}\")\n",
        "\n",
        "    print(\"\\nBest Top-1 Accuracy:\")\n",
        "    print(f\"   Epoch: {df.loc[best_top1_idx, 'epoch']:.0f}\")\n",
        "    print(f\"   Top-1 Acc: {df.loc[best_top1_idx, 'metrics/accuracy_top1']:.4f}\")\n",
        "    print(f\"   Top-5 Acc: {df.loc[best_top1_idx, 'metrics/accuracy_top5']:.4f}\")\n",
        "    print(f\"   Val Loss: {df.loc[best_top1_idx, 'val/loss']:.4f}\")\n",
        "    print(f\"   Train Loss: {df.loc[best_top1_idx, 'train/loss']:.4f}\")\n",
        "\n",
        "    print(\"\\nBest Top-5 Accuracy:\")\n",
        "    print(f\"   Epoch: {df.loc[best_top5_idx, 'epoch']:.0f}\")\n",
        "    print(f\"   Top-5 Acc: {df.loc[best_top5_idx, 'metrics/accuracy_top5']:.4f}\")\n",
        "    print(f\"   Top-1 Acc: {df.loc[best_top5_idx, 'metrics/accuracy_top1']:.4f}\")\n",
        "    print(f\"   Val Loss: {df.loc[best_top5_idx, 'val/loss']:.4f}\")\n",
        "\n",
        "    final_idx = df.index[-1]\n",
        "    print(\"\\nFinal Epoch Metrics:\")\n",
        "    print(f\"   Epoch: {df.loc[final_idx, 'epoch']:.0f}\")\n",
        "    print(f\"   Train Loss: {df.loc[final_idx, 'train/loss']:.4f}\")\n",
        "    print(f\"   Val Loss: {df.loc[final_idx, 'val/loss']:.4f}\")\n",
        "    print(f\"   Top-1 Acc: {df.loc[final_idx, 'metrics/accuracy_top1']:.4f}\")\n",
        "    print(f\"   Top-5 Acc: {df.loc[final_idx, 'metrics/accuracy_top5']:.4f}\")\n",
        "\n",
        "    final_loss_gap = df.loc[final_idx, 'train/loss'] - df.loc[final_idx, 'val/loss']\n",
        "    print(\"\\nOverfitting Analysis:\")\n",
        "    print(f\"   Loss Gap (Train - Val): {final_loss_gap:.4f}\")\n",
        "    if final_loss_gap < -0.1:\n",
        "        print(\"   Status: Good - Model generalizes well\")\n",
        "    elif final_loss_gap < 0.1:\n",
        "        print(\"   Status: Slight underfitting/good fit\")\n",
        "    else:\n",
        "        print(\"   Status: Overfitting detected\")\n",
        "\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def plot_training_metrics(results_csv_path, save_path=\"training_metrics.png\"):\n",
        "    df = pd.read_csv(results_csv_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    sns.set_style(\"whitegrid\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    fig.suptitle('Training Metrics Overview', fontsize=16, fontweight='bold')\n",
        "\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.plot(df['epoch'], df['train/loss'], label='Train Loss', linewidth=2.5, color='#2E86AB', marker='o', markersize=4)\n",
        "    ax1.plot(df['epoch'], df['val/loss'], label='Val Loss', linewidth=2.5, color='#A23B72', marker='s', markersize=4)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Training vs Validation Loss', fontsize=13, fontweight='bold')\n",
        "    ax1.legend(fontsize=11, loc='best')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    best_val_idx = df['val/loss'].idxmin()\n",
        "    ax1.scatter(df.loc[best_val_idx, 'epoch'], df.loc[best_val_idx, 'val/loss'],\n",
        "                color='red', s=150, zorder=5, marker='*', edgecolors='black', linewidth=1.5,\n",
        "                label=f\"Best Val Loss: {df.loc[best_val_idx, 'val/loss']:.4f}\")\n",
        "    ax1.legend(fontsize=10, loc='best')\n",
        "\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.plot(df['epoch'], df['metrics/accuracy_top1'], label='Top-1 Accuracy',\n",
        "             linewidth=2.5, color='#06A77D', marker='o', markersize=4)\n",
        "    ax2.plot(df['epoch'], df['metrics/accuracy_top5'], label='Top-5 Accuracy',\n",
        "             linewidth=2.5, color='#F18F01', marker='s', markersize=4)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Accuracy Metrics', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(fontsize=11, loc='best')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim([0, 1.05])\n",
        "\n",
        "    best_acc_idx = df['metrics/accuracy_top1'].idxmax()\n",
        "    ax2.scatter(df.loc[best_acc_idx, 'epoch'], df.loc[best_acc_idx, 'metrics/accuracy_top1'],\n",
        "                color='red', s=150, zorder=5, marker='*', edgecolors='black', linewidth=1.5,\n",
        "                label=f\"Best Top-1: {df.loc[best_acc_idx, 'metrics/accuracy_top1']:.4f}\")\n",
        "    ax2.legend(fontsize=10, loc='best')\n",
        "\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.plot(df['epoch'], df['lr/pg0'], linewidth=2.5, color='#D62828', marker='o', markersize=4)\n",
        "    ax3.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    ax3.set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
        "    ax3.set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.set_yscale('log')\n",
        "\n",
        "    ax4 = axes[1, 1]\n",
        "    loss_gap = df['train/loss'] - df['val/loss']\n",
        "    ax4.plot(df['epoch'], loss_gap, linewidth=2.5, color='#C1121F', marker='o', markersize=4)\n",
        "    ax4.axhline(y=0, color='black', linestyle='--', linewidth=2, alpha=0.7)\n",
        "    ax4.fill_between(df['epoch'], loss_gap, 0, where=(loss_gap < 0),\n",
        "                      alpha=0.3, color='green', label='Generalizing Well')\n",
        "    ax4.fill_between(df['epoch'], loss_gap, 0, where=(loss_gap > 0),\n",
        "                      alpha=0.3, color='red', label='Overfitting')\n",
        "    ax4.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    ax4.set_ylabel('Train Loss - Val Loss', fontsize=12, fontweight='bold')\n",
        "    ax4.set_title('Overfitting Detection', fontsize=13, fontweight='bold')\n",
        "    ax4.legend(fontsize=10, loc='best')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Plot saved to: {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def analyze_training_progress(results_csv_path):\n",
        "    df = pd.read_csv(results_csv_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING PROGRESS ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    initial_val_loss = df.loc[0, 'val/loss']\n",
        "    final_val_loss = df.loc[df.index[-1], 'val/loss']\n",
        "    val_loss_improvement = ((initial_val_loss - final_val_loss) / initial_val_loss) * 100\n",
        "\n",
        "    initial_acc = df.loc[0, 'metrics/accuracy_top1']\n",
        "    final_acc = df.loc[df.index[-1], 'metrics/accuracy_top1']\n",
        "    acc_improvement = ((final_acc - initial_acc) / initial_acc) * 100\n",
        "\n",
        "    print(f\"\\nOverall Improvements:\")\n",
        "    print(f\"   Val Loss: {initial_val_loss:.4f} -> {final_val_loss:.4f} ({val_loss_improvement:+.2f}%)\")\n",
        "    print(f\"   Top-1 Acc: {initial_acc:.4f} -> {final_acc:.4f} ({acc_improvement:+.2f}%)\")\n",
        "\n",
        "    last_10_epochs = df.tail(min(10, len(df)))\n",
        "    val_loss_std = last_10_epochs['val/loss'].std()\n",
        "    acc_std = last_10_epochs['metrics/accuracy_top1'].std()\n",
        "\n",
        "    print(f\"\\nLast {len(last_10_epochs)} Epochs Stability:\")\n",
        "    print(f\"   Val Loss Std Dev: {val_loss_std:.4f}\")\n",
        "    print(f\"   Accuracy Std Dev: {acc_std:.4f}\")\n",
        "\n",
        "    if val_loss_std < 0.01 and acc_std < 0.01:\n",
        "        print(\"   Status: Training has plateaued\")\n",
        "    else:\n",
        "        print(\"   Status: Still improving\")\n",
        "\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "\n",
        "import glob\n",
        "\n",
        "result_dirs = sorted(glob.glob('runs/classify/train*'), key=lambda x: Path(x).stat().st_mtime)\n",
        "\n",
        "if result_dirs:\n",
        "    latest_result = result_dirs[-1]\n",
        "    results_csv = Path(latest_result) / \"results.csv\"\n",
        "\n",
        "    print(f\"Using results from: {latest_result}\\n\")\n",
        "\n",
        "    if results_csv.exists():\n",
        "        df = print_best_metrics(results_csv)\n",
        "        analyze_training_progress(results_csv)\n",
        "        save_plot = Path(latest_result) / \"training_analysis.png\"\n",
        "        plot_training_metrics(results_csv, save_path=save_plot)\n",
        "    else:\n",
        "        print(f\"results.csv not found at {results_csv}\")\n",
        "else:\n",
        "    print(\"No training results found. Please check your training directory.\")"
      ],
      "metadata": {
        "id": "LkVUWvSjQUYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediksi**"
      ],
      "metadata": {
        "id": "16w_QCrVSQJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"/content/drive/MyDrive/data/MangoLeafBD_Split/runs/classify/train/weights/best_saved_model/best_float32.tflite\")  # load a custom model\n",
        "\n",
        "# Predict with the model\n",
        "results = model(\"/content/drive/MyDrive/data/MangoLeafBD_Split/test/Die Back/IMG_20211028_003845 (Custom).jpg\")  # predict on an image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7I6YSYnSUg3",
        "outputId": "38d99615-5587-47e5-c7ec-2629f99cb348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/drive/MyDrive/data/MangoLeafBD_Split/runs/classify/train/weights/best_saved_model/best_float32.tflite for TensorFlow Lite inference...\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/data/MangoLeafBD_Split/test/Die Back/IMG_20211028_003845 (Custom).jpg: 224x224 Die Back 0.60, Sooty Mould 0.18, Gall Midge 0.12, Healthy 0.04, Powdery Mildew 0.03, 8.4ms\n",
            "Speed: 4.8ms preprocess, 8.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export dan Convert ke Tflite**"
      ],
      "metadata": {
        "id": "yxso39nrVweC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load model hasil training\n",
        "model = YOLO(\"/content/drive/MyDrive/data/MangoLeafBD_Split/runs/classify/train/weights/best.pt\")\n",
        "\n",
        "# Export ke format TFLite dengan quantization INT8\n",
        "model.export(format=\"tflite\", int8=True)"
      ],
      "metadata": {
        "id": "JsvR2x-BV1L0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}